{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9150287d-530a-4d75-88dd-8179cf66fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "import files\n",
    "importlib.reload(files)\n",
    "\n",
    "import fonctions\n",
    "importlib.reload(fonctions)\n",
    "\n",
    "from files import *\n",
    "from fonctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824f222d-e9df-44b3-9e84-c1b4e5b65b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "consommation_chauffage_toulouse = extract_and_concat_consommation(toulouse, column_index=4, prefix=\"consommation_heat_\")\n",
    "consommation_chauffage_zurich = extract_and_concat_consommation(zurich, column_index=4, prefix=\"consommation_heat_\")\n",
    "consommation_chauffage_seville = extract_and_concat_consommation(seville, column_index=4, prefix=\"consommation_heat_\")\n",
    "consommation_climatisation_toulouse = extract_and_concat_consommation(toulouse, column_index=5, prefix=\"consommation_cool_\")\n",
    "consommation_climatisation_zurich = extract_and_concat_consommation(zurich, column_index=5, prefix=\"consommation_cool_\")\n",
    "consommation_climatisation_seville = extract_and_concat_consommation(seville, column_index=5, prefix=\"consommation_cool_\")\n",
    "\n",
    "\n",
    "city_groups = {\n",
    "    \"toulouse\": toulouse_meteo,\n",
    "    \"zurich\": zurich_meteo,\n",
    "    \"seville\": seville_meteo\n",
    "}\n",
    "\n",
    "prefix_column_map = {\n",
    "    \"Text_\": 1,\n",
    "    \"Hum_\": 3,\n",
    "    \"Wind_\": 4,\n",
    "    \"Solar_\": 5,\n",
    "    \"Ground_\": 10\n",
    "}\n",
    "\n",
    "combined_data = extract_and_combine_all(city_groups, prefix_column_map)\n",
    "\n",
    "Text_combined_toulouse = combined_data.get('Text_combined_toulouse')\n",
    "Hum_combined_toulouse = combined_data.get('Hum_combined_toulouse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a756cb-3ee3-4cc2-9602-4d893bb6afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Encodage positionnel pour donner une notion du temps au modèle\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Le modèle Transformer principal\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, num_features=3, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1, output_size=24):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(num_features, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, output_size)\n",
    "        #self.output_layer = nn.Linear(d_model, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len=24, num_features=3)\n",
    "        x = self.input_projection(x)         # → (batch_size, 24, d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)      # → (batch_size, 24, d_model)\n",
    "        out = self.output_layer(x)           # → (batch_size, 24, 24)\n",
    "        return out[:, :, 0]                  # → (batch_size, 24), on garde une seule sortie par heure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41910b3e-4dcd-463d-9bc3-38e19fab718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, input_blocks, target_block):\n",
    "        \"\"\"\n",
    "        df : DataFrame avec toutes les variables concaténées par blocs de 24h\n",
    "        input_blocks : liste de tuples (col_start, col_end, day_offset)\n",
    "                       où day_offset = 0 pour aujourd’hui, -1 pour hier, etc.\n",
    "        target_block : tuple (col_start, col_end) pour la target (conso aujourd’hui)\n",
    "        \"\"\"\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "\n",
    "        for i in range(1, len(df)):  # Commencer à 1 pour avoir accès à hier\n",
    "            input_seq = []\n",
    "\n",
    "            for start, end, offset in input_blocks:\n",
    "                row_index = i + offset\n",
    "                values = df.iloc[row_index, start:end].values  # shape: (24,)\n",
    "                input_seq.append(values)\n",
    "\n",
    "            # shape finale: (24, num_features)\n",
    "            input_seq = np.stack(input_seq, axis=1)\n",
    "            self.x.append(input_seq)\n",
    "\n",
    "            # Target: consommation aujourd’hui\n",
    "            target = df.iloc[i, target_block[0]:target_block[1]].values\n",
    "            self.y.append(target)\n",
    "\n",
    "        self.x = torch.tensor(self.x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(self.y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c17e67-9645-4834-a3d7-1b454e450faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a87e9d48-dd90-4150-bb72-d37b30b84ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Fonction pour calculer les métriques ---\n",
    "def compute_metrics(predictions, targets):\n",
    "    mse = torch.mean((predictions - targets) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    mae = torch.mean(torch.abs(predictions - targets))\n",
    "    ss_total = torch.sum((targets - torch.mean(targets)) ** 2)\n",
    "    ss_residual = torch.sum((targets - predictions) ** 2)\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "    cvrmse = (rmse / torch.mean(targets)) * 100\n",
    "    return mae, rmse, mse, r2, cvrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc0f7e53-863c-4421-9737-110ab5b595c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fonction pour séparer les données ---\n",
    "def prepare_data(df_scaled,n_features):\n",
    "\n",
    "    input_blocks = []\n",
    "\n",
    "    for i in range(n_features):\n",
    "        start = i * 24\n",
    "        end = (i + 1) * 24\n",
    "\n",
    "        # Aujourd’hui\n",
    "        input_blocks.append((start, end, 0))\n",
    "        # Hier\n",
    "        input_blocks.append((start, end, -1))\n",
    "\n",
    "    # Ajouter la consommation d’hier (toujours la dernière feature)\n",
    "    conso_start = n_features * 24\n",
    "    conso_end = conso_start + 24\n",
    "    input_blocks.append((conso_start, conso_end, -1))\n",
    "    target_block = (df_scaled.shape[1] - 24, df_scaled.shape[1])    \n",
    "        \n",
    "    df_trainval, df_test = train_test_split(df_scaled, test_size=0.2, shuffle=False)\n",
    "    df_train, df_val = train_test_split(df_trainval, test_size=0.1, shuffle=False)\n",
    "    \n",
    "    train_dataset = TimeSeriesDataset(df_train.reset_index(drop=True),input_blocks, target_block)\n",
    "    val_dataset = TimeSeriesDataset(df_val.reset_index(drop=True),input_blocks, target_block)\n",
    "    test_dataset = TimeSeriesDataset(df_test.reset_index(drop=True),input_blocks, target_block)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "767bd927-7ff6-4c73-907f-9487506a6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Fonction pour entraîner le modèle ---\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = total_mae = total_rmse = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            mae, rmse, _, _, _ = compute_metrics(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae.item()\n",
    "            total_rmse += rmse.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = val_mae = val_rmse = 0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                output = model(x_val)\n",
    "                loss = criterion(output, y_val)\n",
    "                mae, rmse, _, _, _ = compute_metrics(output, y_val)\n",
    "                val_loss += loss.item()\n",
    "                val_mae += mae.item()\n",
    "                val_rmse += rmse.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {total_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}, Val MAE: {val_mae / len(val_loader):.4f}, Val RMSE: {val_rmse / len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c14b825-5df1-4cfb-be19-e08af9998d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fonction pour évaluer sur le test set ---\n",
    "def evaluate_test(model, test_loader, scaler_y):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test in test_loader:\n",
    "            y_pred = model(x_test)\n",
    "            all_preds.append(y_pred.squeeze().cpu().numpy())\n",
    "            all_targets.append(y_test.squeeze().cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds).reshape(-1, 24)\n",
    "    all_targets = np.array(all_targets).reshape(-1, 24)\n",
    "\n",
    "    # Inverse scaling\n",
    "    all_preds_orig = scaler_y.inverse_transform(all_preds)\n",
    "    all_targets_orig = scaler_y.inverse_transform(all_targets)\n",
    "\n",
    "    # Convertir en tensors\n",
    "    y_pred_tensor = torch.tensor(all_preds_orig)\n",
    "    y_test_tensor = torch.tensor(all_targets_orig)\n",
    "\n",
    "    # Calculer les métriques\n",
    "    mae, rmse, mse, r2, cvrmse = compute_metrics(y_pred_tensor, y_test_tensor)\n",
    "    print(f\"Test Metrics -> MAE: {mae:.4f}, RMSE: {rmse:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}, CVRMSE: {cvrmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "494f35f7-2fd1-42ff-84fb-fc74535889fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toubia\\AppData\\Local\\Temp\\ipykernel_1636\\4231738616.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  self.x = torch.tensor(self.x, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.0712, Val Loss: 0.0325, Val MAE: 0.1499, Val RMSE: 0.1793\n",
      "Epoch 2/10, Train Loss: 0.0201, Val Loss: 0.0274, Val MAE: 0.1355, Val RMSE: 0.1640\n",
      "Epoch 3/10, Train Loss: 0.0167, Val Loss: 0.0256, Val MAE: 0.1275, Val RMSE: 0.1577\n",
      "Epoch 4/10, Train Loss: 0.0146, Val Loss: 0.0233, Val MAE: 0.1227, Val RMSE: 0.1509\n",
      "Epoch 5/10, Train Loss: 0.0132, Val Loss: 0.0224, Val MAE: 0.1206, Val RMSE: 0.1486\n",
      "Epoch 6/10, Train Loss: 0.0119, Val Loss: 0.0203, Val MAE: 0.1135, Val RMSE: 0.1409\n",
      "Epoch 7/10, Train Loss: 0.0105, Val Loss: 0.0176, Val MAE: 0.1045, Val RMSE: 0.1313\n",
      "Epoch 8/10, Train Loss: 0.0097, Val Loss: 0.0179, Val MAE: 0.1044, Val RMSE: 0.1323\n",
      "Epoch 9/10, Train Loss: 0.0088, Val Loss: 0.0161, Val MAE: 0.1008, Val RMSE: 0.1256\n",
      "Epoch 10/10, Train Loss: 0.0082, Val Loss: 0.0153, Val MAE: 0.0948, Val RMSE: 0.1219\n",
      "Test Metrics -> MAE: 149.8173, RMSE: 255.2692, MSE: 65162.3828, R2: 0.8750, CVRMSE: 98.8095\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "prediction en fonction de Text et conso t-1 + text  t \n",
    "\"\"\"\n",
    "\n",
    "df2 = Text_combined_toulouse.copy()\n",
    "df2=pd.concat([df2,consommation_chauffage_toulouse],axis=1).reset_index(drop=True)\n",
    "df2.columns = [f\"col_{i}\" for i in range(df2.shape[1])]\n",
    "\n",
    "target_columns = df2.columns[-24:]  \n",
    "input_columns = df2.columns[:-24]\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler_X.fit_transform(df2[input_columns]), columns=input_columns)\n",
    "y_scaled = pd.DataFrame(scaler_y.fit_transform(df2[target_columns]), columns=target_columns)\n",
    "df_scaled = pd.concat([X_scaled, y_scaled], axis=1)\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_data(df_scaled,1)\n",
    "\n",
    "\n",
    "model = TimeSeriesTransformer(num_features=3, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1, output_size=24) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10)\n",
    "evaluate_test(model, test_loader, scaler_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac19a81-2587-4b92-bced-134c5fbfb115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.0900, Val Loss: 0.0368, Val MAE: 0.1636, Val RMSE: 0.1912\n",
      "Epoch 2/10, Train Loss: 0.0257, Val Loss: 0.0287, Val MAE: 0.1421, Val RMSE: 0.1678\n",
      "Epoch 3/10, Train Loss: 0.0180, Val Loss: 0.0252, Val MAE: 0.1303, Val RMSE: 0.1575\n",
      "Epoch 4/10, Train Loss: 0.0142, Val Loss: 0.0225, Val MAE: 0.1196, Val RMSE: 0.1486\n",
      "Epoch 5/10, Train Loss: 0.0129, Val Loss: 0.0195, Val MAE: 0.1098, Val RMSE: 0.1381\n",
      "Epoch 6/10, Train Loss: 0.0108, Val Loss: 0.0165, Val MAE: 0.0994, Val RMSE: 0.1269\n",
      "Epoch 7/10, Train Loss: 0.0088, Val Loss: 0.0147, Val MAE: 0.0927, Val RMSE: 0.1200\n",
      "Epoch 8/10, Train Loss: 0.0078, Val Loss: 0.0135, Val MAE: 0.0872, Val RMSE: 0.1148\n",
      "Epoch 9/10, Train Loss: 0.0074, Val Loss: 0.0132, Val MAE: 0.0870, Val RMSE: 0.1138\n",
      "Epoch 10/10, Train Loss: 0.0068, Val Loss: 0.0133, Val MAE: 0.0835, Val RMSE: 0.1137\n",
      "Test Metrics -> MAE: 138.6771, RMSE: 234.8675, MSE: 55162.7617, R2: 0.8942, CVRMSE: 90.9124\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "prediction en fonction de Text et hum  et conso t-1 + text et hum  t \n",
    "\"\"\"\n",
    "\n",
    "Text_Hum_combined_toulouse=concat_and_create_final_df(\"toulouse\",  ['Text_', 'Hum_'],combined_data)\n",
    "df2 = Text_Hum_combined_toulouse.copy()\n",
    "df2=pd.concat([df2,consommation_chauffage_toulouse],axis=1).reset_index(drop=True)\n",
    "df2.columns = [f\"col_{i}\" for i in range(df2.shape[1])]\n",
    "\n",
    "target_columns = df2.columns[-24:]  \n",
    "input_columns = df2.columns[:-24]\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler_X.fit_transform(df2[input_columns]), columns=input_columns)\n",
    "y_scaled = pd.DataFrame(scaler_y.fit_transform(df2[target_columns]), columns=target_columns)\n",
    "df_scaled = pd.concat([X_scaled, y_scaled], axis=1)\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_data(df_scaled,2)\n",
    "\n",
    "\n",
    "model = TimeSeriesTransformer(num_features=5, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1, output_size=24) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10)\n",
    "evaluate_test(model, test_loader, scaler_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7be8f663-b309-44d2-9232-96f7a3ddeda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.0877, Val Loss: 0.0446, Val MAE: 0.1713, Val RMSE: 0.2067\n",
      "Epoch 2/10, Train Loss: 0.0291, Val Loss: 0.0322, Val MAE: 0.1431, Val RMSE: 0.1762\n",
      "Epoch 3/10, Train Loss: 0.0193, Val Loss: 0.0267, Val MAE: 0.1330, Val RMSE: 0.1614\n",
      "Epoch 4/10, Train Loss: 0.0163, Val Loss: 0.0250, Val MAE: 0.1292, Val RMSE: 0.1563\n",
      "Epoch 5/10, Train Loss: 0.0141, Val Loss: 0.0226, Val MAE: 0.1190, Val RMSE: 0.1483\n",
      "Epoch 6/10, Train Loss: 0.0125, Val Loss: 0.0209, Val MAE: 0.1162, Val RMSE: 0.1430\n",
      "Epoch 7/10, Train Loss: 0.0107, Val Loss: 0.0179, Val MAE: 0.1054, Val RMSE: 0.1320\n",
      "Epoch 8/10, Train Loss: 0.0101, Val Loss: 0.0166, Val MAE: 0.1010, Val RMSE: 0.1271\n",
      "Epoch 9/10, Train Loss: 0.0097, Val Loss: 0.0161, Val MAE: 0.0987, Val RMSE: 0.1252\n",
      "Epoch 10/10, Train Loss: 0.0092, Val Loss: 0.0144, Val MAE: 0.0916, Val RMSE: 0.1180\n",
      "Test Metrics -> MAE: 132.0270, RMSE: 256.6242, MSE: 65855.9688, R2: 0.8737, CVRMSE: 99.3339\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "prediction en fonction de( Text et hum et wind  et conso )t-1 + (text  et hum et wind ) t \n",
    "\"\"\"\n",
    "\n",
    "Text_Hum_combined_toulouse=concat_and_create_final_df(\"toulouse\",  ['Text_', 'Hum_','Wind_'],combined_data)\n",
    "\n",
    "\n",
    "df2 = Text_Hum_combined_toulouse.copy()\n",
    "df2=pd.concat([df2,consommation_chauffage_toulouse],axis=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df2.columns = [f\"col_{i}\" for i in range(df2.shape[1])]\n",
    "\n",
    "target_columns = df2.columns[-24:]  \n",
    "input_columns = df2.columns[:-24]\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler_X.fit_transform(df2[input_columns]), columns=input_columns)\n",
    "y_scaled = pd.DataFrame(scaler_y.fit_transform(df2[target_columns]), columns=target_columns)\n",
    "df_scaled = pd.concat([X_scaled, y_scaled], axis=1)\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_data(df_scaled,3)\n",
    "\n",
    "\n",
    "model = TimeSeriesTransformer(num_features=7, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1, output_size=24) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10)\n",
    "evaluate_test(model, test_loader, scaler_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada201b-933f-43e5-b394-f33f137e06c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45de5da0-e493-479e-9ec8-855622f86e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Étape 1 : clustering\n",
    "clustering_heat_toulouse = clustering(\n",
    "    df=consommation_chauffage_toulouse,  \n",
    "    n_parts=1,                          \n",
    "    status_column=\"heat_on\",           \n",
    "    n_clusters_list=[3]                \n",
    ")\n",
    "\n",
    "# Étape 2 : concaténer les données\n",
    "df2 = Text_combined_toulouse.copy()\n",
    "df2 = pd.concat([df2, clustering_heat_toulouse], axis=1).reset_index(drop=True)\n",
    "df2.columns = [f\"col_{i}\" for i in range(df2.shape[1])]\n",
    "\n",
    "\n",
    "# Étape 3 : Séparer colonnes input/target\n",
    "target_columns = df2.columns[-26:-2]  \n",
    "input_columns = df2.columns[:24]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(df2[input_columns]), columns=input_columns)\n",
    "y_scaled = pd.DataFrame(scaler.fit_transform(df2[target_columns]), columns=target_columns)\n",
    "\n",
    "status = pd.DataFrame(np.tile(df2.iloc[:, -2].values, (24, 1)).T, columns=[f\"status_{i}\" for i in range(24)])\n",
    "cluster = pd.DataFrame(np.tile(df2.iloc[:, -1].values, (24, 1)).T, columns=[f\"cluster_{i}\" for i in range(24)])\n",
    "df_scaled = pd.concat([X_scaled, status, cluster, y_scaled], axis=1).reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f518a75-1918-4f23-9671-851fd3b78e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3512, Val Loss: 0.6471, Val MAE: 0.6013, Val RMSE: 0.7925\n",
      "Epoch 2/10, Train Loss: 0.2259, Val Loss: 0.4612, Val MAE: 0.5200, Val RMSE: 0.6693\n",
      "Epoch 3/10, Train Loss: 0.1628, Val Loss: 0.4311, Val MAE: 0.5116, Val RMSE: 0.6510\n",
      "Epoch 4/10, Train Loss: 0.1505, Val Loss: 0.4250, Val MAE: 0.4650, Val RMSE: 0.6416\n",
      "Epoch 5/10, Train Loss: 0.1294, Val Loss: 0.3395, Val MAE: 0.4100, Val RMSE: 0.5734\n",
      "Epoch 6/10, Train Loss: 0.1133, Val Loss: 0.3179, Val MAE: 0.3779, Val RMSE: 0.5548\n",
      "Epoch 7/10, Train Loss: 0.1009, Val Loss: 0.3301, Val MAE: 0.3840, Val RMSE: 0.5608\n",
      "Epoch 8/10, Train Loss: 0.0932, Val Loss: 0.2459, Val MAE: 0.3259, Val RMSE: 0.4854\n",
      "Epoch 9/10, Train Loss: 0.0780, Val Loss: 0.2652, Val MAE: 0.3667, Val RMSE: 0.5045\n",
      "Epoch 10/10, Train Loss: 0.0716, Val Loss: 0.2890, Val MAE: 0.3619, Val RMSE: 0.5204\n",
      "Test Metrics -> MAE: 95.6262, RMSE: 190.6623, MSE: 36352.1289, R2: 0.9303, CVRMSE: 73.8015\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "prediction en fonction de (Text et conso et cluster et status) t-1 + (Text  et cluster et status)  t \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_data(df_scaled,3)\n",
    "\n",
    "model = TimeSeriesTransformer(num_features=7, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1, output_size=24) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10)\n",
    "evaluate_test(model, test_loader, scaler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64f597-5575-4752-b38b-45af63bb4f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
