{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dff150d-4e3b-481c-88eb-63239266e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "import files\n",
    "importlib.reload(files)\n",
    "\n",
    "import fonctions\n",
    "importlib.reload(fonctions)\n",
    "\n",
    "from files import *\n",
    "from fonctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a37c0c2-47a9-4c8a-b873-4d9dbe70bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "consommation_chauffage_toulouse = extract_and_concat_consommation(toulouse, column_index=4, prefix=\"consommation_heat_\")\n",
    "consommation_chauffage_zurich = extract_and_concat_consommation(zurich, column_index=4, prefix=\"consommation_heat_\")\n",
    "consommation_chauffage_seville = extract_and_concat_consommation(seville, column_index=4, prefix=\"consommation_heat_\")\n",
    "consommation_climatisation_toulouse = extract_and_concat_consommation(toulouse, column_index=5, prefix=\"consommation_cool_\")\n",
    "consommation_climatisation_zurich = extract_and_concat_consommation(zurich, column_index=5, prefix=\"consommation_cool_\")\n",
    "consommation_climatisation_seville = extract_and_concat_consommation(seville, column_index=5, prefix=\"consommation_cool_\")\n",
    "\n",
    "\n",
    "city_groups = {\n",
    "    \"toulouse\": toulouse_meteo,\n",
    "    \"zurich\": zurich_meteo,\n",
    "    \"seville\": seville_meteo\n",
    "}\n",
    "\n",
    "prefix_column_map = {\n",
    "    \"Text_\": 1,\n",
    "    \"Hum_\": 3,\n",
    "    \"Wind_\": 4,\n",
    "    \"Solar_\": 5,\n",
    "    \"Ground_\": 10\n",
    "}\n",
    "\n",
    "combined_data = extract_and_combine_all(city_groups, prefix_column_map)\n",
    "\n",
    "Text_combined_toulouse = combined_data.get('Text_combined_toulouse')\n",
    "Hum_combined_toulouse = combined_data.get('Hum_combined_toulouse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ada14cb-b43b-4019-81e0-183ab26064ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Fonction pour calculer les métriques ---\n",
    "def compute_metrics(predictions, targets):\n",
    "    mse = torch.mean((predictions - targets) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    mae = torch.mean(torch.abs(predictions - targets))\n",
    "    ss_total = torch.sum((targets - torch.mean(targets)) ** 2)\n",
    "    ss_residual = torch.sum((targets - predictions) ** 2)\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "    cvrmse = (rmse / torch.mean(targets)) * 100\n",
    "    return mae, rmse, mse, r2, cvrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2653c330-add9-4611-93c2-0aa74704bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "48 heures (24 t-1 + 24 t) avec 4 features)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Dataset construit à partir d'un DataFrame df structuré en blocs de 24 colonnes :\n",
    "        - Température : colonnes 0 à 23\n",
    "        - Chauffage : colonnes 24 à 47\n",
    "        - Clusters : colonnes 48 à 71\n",
    "        - Consommation : colonnes 72 à 95\n",
    "        \"\"\"\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "\n",
    "        for i in range(1, len(df)):  # On commence à partir de i=1 pour avoir un i-1\n",
    "            # Données pour l'instant t-1\n",
    "            prev_temp = df.iloc[i-1, :24].values\n",
    "            prev_heat = df.iloc[i-1, 24:48].values\n",
    "            prev_cluster = df.iloc[i-1, 48:72].values\n",
    "            prev_conso = df.iloc[i-1, 72:].values\n",
    "\n",
    "            # Rassembler les données de température, chauffage, cluster et consommation passées dans un vecteur\n",
    "            prev_data = np.column_stack([prev_temp, prev_heat, prev_cluster, prev_conso])\n",
    "\n",
    "            # Données pour l'instant t (pour chaque heure t, mais on met la consommation à zéro)\n",
    "            curr_temp = df.iloc[i, :24].values\n",
    "            curr_heat = df.iloc[i, 24:48].values\n",
    "            curr_cluster = df.iloc[i, 48:72].values\n",
    "            curr_conso_zero = np.zeros(24)\n",
    "\n",
    "            # Rassembler les données actuelles de température, chauffage et cluster, et consommation à zéro\n",
    "            curr_data = np.column_stack([curr_temp, curr_heat, curr_cluster, curr_conso_zero])\n",
    "\n",
    "            # Ajouter les deux blocs dans x\n",
    "            self.x.append(np.vstack([prev_data, curr_data]))  # Shape : (48, 4)\n",
    "\n",
    "            # La cible (consommation réelle pour l'instant t)\n",
    "            target = df.iloc[i, 72:].values  # Consommation réelle\n",
    "            self.y.append(target)  # Shape : (24,)\n",
    "\n",
    "        self.x = torch.tensor(self.x, dtype=torch.float32)  # Shape: (N, 48, 4)\n",
    "        self.y = torch.tensor(self.y, dtype=torch.float32)  # Shape: (N, 24)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def prepare_data(df_scaled):\n",
    "    \"\"\"\n",
    "    Prépare les DataLoaders pour entraînement, validation et test\n",
    "    en utilisant la nouvelle version de TimeSeriesDataset.\n",
    "    \"\"\"\n",
    "    # Découpage en train, validation et test\n",
    "    df_trainval, df_test = train_test_split(df_scaled, test_size=0.2, shuffle=False)\n",
    "    df_train, df_val = train_test_split(df_trainval, test_size=0.1, shuffle=False)\n",
    "\n",
    "    # Création des datasets (la nouvelle version de TimeSeriesDataset ne prend que df)\n",
    "    train_dataset = TimeSeriesDataset(df_train.reset_index(drop=True))\n",
    "    val_dataset = TimeSeriesDataset(df_val.reset_index(drop=True))\n",
    "    test_dataset = TimeSeriesDataset(df_test.reset_index(drop=True))\n",
    "\n",
    "    # Création des DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86241e2-5b64-4cb5-a59f-dea886054345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "# Encodage positionnel pour donner une notion du temps au modèle\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Le modèle Transformer principal\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, num_features=3, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1, output_size=24):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(num_features, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, output_size)\n",
    "        #self.output_layer = nn.Linear(d_model, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len=24, num_features=3)\n",
    "        x = self.input_projection(x)         # → (batch_size, 24, d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)      # → (batch_size, 24, d_model)\n",
    "        out = self.output_layer(x)           # → (batch_size, 24, 24)\n",
    "        return out[:, -24:, 0]                  # → (batch_size, 24), on garde une seule sortie par heure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "650b7a10-9e24-4c65-8bef-93835e10d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Fonction pour entraîner le modèle ---\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = total_mae = total_rmse = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            mae, rmse, _, _, _ = compute_metrics(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae.item()\n",
    "            total_rmse += rmse.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = val_mae = val_rmse = 0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                output = model(x_val)\n",
    "                loss = criterion(output, y_val)\n",
    "                mae, rmse, _, _, _ = compute_metrics(output, y_val)\n",
    "                val_loss += loss.item()\n",
    "                val_mae += mae.item()\n",
    "                val_rmse += rmse.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {total_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}, Val MAE: {val_mae / len(val_loader):.4f}, Val RMSE: {val_rmse / len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b960c367-ca11-46cb-9160-e151313ae443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, test_loader, scaler_y):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test in test_loader:\n",
    "            y_pred = model(x_test)\n",
    "            all_preds.append(y_pred.squeeze().cpu().numpy())\n",
    "            all_targets.append(y_test.squeeze().cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds).reshape(-1, 24)\n",
    "    all_targets = np.array(all_targets).reshape(-1, 24)\n",
    "\n",
    "    # Inverse scaling\n",
    "    all_preds_orig = scaler_y.inverse_transform(all_preds)\n",
    "    all_targets_orig = scaler_y.inverse_transform(all_targets)\n",
    "\n",
    "    # Convertir en tensors\n",
    "    y_pred_tensor = torch.tensor(all_preds_orig)\n",
    "    y_test_tensor = torch.tensor(all_targets_orig)\n",
    "\n",
    "    # Calculer les métriques\n",
    "    mae, rmse, mse, r2, cvrmse = compute_metrics(y_pred_tensor, y_test_tensor)\n",
    "    print(f\"Test Metrics -> MAE: {mae:.4f}, RMSE: {rmse:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}, CVRMSE: {cvrmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8702d45e-a7cf-4239-b6bd-7747d987d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Étape 1 : clustering\n",
    "clustering_heat_toulouse = clustering(\n",
    "    df=consommation_chauffage_toulouse,  \n",
    "    n_parts=1,                          \n",
    "    status_column=\"heat_on\",           \n",
    "    n_clusters_list=[3]                \n",
    ")\n",
    "\n",
    "# Étape 2 : concaténer les données\n",
    "df2 = Text_combined_toulouse.copy()\n",
    "df2 = pd.concat([df2, clustering_heat_toulouse], axis=1).reset_index(drop=True)\n",
    "df2.columns = [f\"col_{i}\" for i in range(df2.shape[1])]\n",
    "\n",
    "\n",
    "# Étape 3 : Séparer colonnes input/target\n",
    "target_columns = df2.columns[-26:-2]  \n",
    "input_columns = df2.columns[:24]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(df2[input_columns]), columns=input_columns)\n",
    "y_scaled = pd.DataFrame(scaler.fit_transform(df2[target_columns]), columns=target_columns)\n",
    "\n",
    "status = pd.DataFrame(np.tile(df2.iloc[:, -2].values, (24, 1)).T, columns=[f\"status_{i}\" for i in range(24)])\n",
    "cluster = pd.DataFrame(np.tile(df2.iloc[:, -1].values, (24, 1)).T, columns=[f\"cluster_{i}\" for i in range(24)])\n",
    "df_scaled = pd.concat([X_scaled, status, cluster, y_scaled], axis=1).reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a63cb205-f9be-4983-bdb6-53327e1e7f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4885, Val Loss: 0.6399, Val MAE: 0.6476, Val RMSE: 0.7887\n",
      "Epoch 2/10, Train Loss: 0.2142, Val Loss: 0.4614, Val MAE: 0.5276, Val RMSE: 0.6703\n",
      "Epoch 3/10, Train Loss: 0.1549, Val Loss: 0.4230, Val MAE: 0.4691, Val RMSE: 0.6426\n",
      "Epoch 4/10, Train Loss: 0.1413, Val Loss: 0.3805, Val MAE: 0.4526, Val RMSE: 0.6087\n",
      "Epoch 5/10, Train Loss: 0.1302, Val Loss: 0.3798, Val MAE: 0.4550, Val RMSE: 0.6072\n",
      "Epoch 6/10, Train Loss: 0.1217, Val Loss: 0.3302, Val MAE: 0.4096, Val RMSE: 0.5652\n",
      "Epoch 7/10, Train Loss: 0.1122, Val Loss: 0.2829, Val MAE: 0.3790, Val RMSE: 0.5219\n",
      "Epoch 8/10, Train Loss: 0.0890, Val Loss: 0.3013, Val MAE: 0.3755, Val RMSE: 0.5356\n",
      "Epoch 9/10, Train Loss: 0.0914, Val Loss: 0.2412, Val MAE: 0.3207, Val RMSE: 0.4742\n",
      "Epoch 10/10, Train Loss: 0.0823, Val Loss: 0.2403, Val MAE: 0.3249, Val RMSE: 0.4740\n",
      "Test Metrics -> MAE: 110.1486, RMSE: 182.6036, MSE: 33344.0859, R2: 0.9361, CVRMSE: 70.6821\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "prediction en fonction de (Text et conso et cluster et status) t-1 + (Text  et cluster et status)  t \n",
    "sur 48 heures 4 features par heures\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_data(df_scaled)\n",
    "model = TimeSeriesTransformer(num_features=4, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1, output_size=24) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10)\n",
    "evaluate_test(model, test_loader, scaler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778f587-641a-4ff2-8732-bd56f28ae866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
